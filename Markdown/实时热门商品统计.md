
 * Maven工程UserBehaviorAnalysis子模块HotItemsAnalysis

### 1.需求

每隔5分钟输出最近一小时内点击量最多的前N个商品
1. 抽取出业务时间戳，告诉Flink框架基于业务时间做窗口
2. 过滤出点击行为数据
3. 按一小时的窗口大小，每5分钟一次，做滑动窗口聚合(Sliding Window)
4. 按每个窗口聚合，输出每个窗口中点击量前N名的成员

### 2.程序主体实现
Flink默认使用ProcessingTime处理，我们需要基于EventTime统计业务时间上的每小时的点击量，显式设置如下
```scala
env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)
```
我们的数据源的数据已经经过处理，事件的时间戳是单调递增的，所以可以将每条数据的业务时间当作watermark，使用`assignAscendingTimestamps`实现时间戳的抽取和Watermark的生成
```scala
.assignAscendingTimestamps(_.timestamps * 1000)
```
 * 实际业务中一般数据为乱序的，不使用`assignAscendingTimestamps`而是使用`BoundedOutOfOrdernessTimestampExtractor`

### 3.过滤出点击事件
由于原始数据中存在点击、购买、收藏、喜欢各种行为的数据，但是我们只需要统计点击量，所以先使用filter将点击行为数据过滤出来。
```scala
.filter(_.behavior == "pv")
```

### 4.设置滑动窗口统计点击量
 * 对商品进行分组

```scala
.keyBy(_.itemId)    
```

 * 每隔五分钟统计一次最近一小时每个商品的点击量，使用滑动窗口实现

```scala
.timeWindow(Time.hours(1), Time.minutes(5))
```

 * 对数据进行增量聚合
 * 使用AggregateFunction提前聚合掉数据，减少state的存储压力
 * 使用WindowFunction将每个key每个窗口聚合后的结果带上其他信息进行输出

```scala
.aggregate(new CountAgg(), new WindowResultFunction())
```
 * AggregateFunction和WindowFunction的具体实现见Maven工程

### 5.计算最热门Top N商品
为了统计每个窗口下最热门的商品，我们需要再次按窗口进行分组，这里根据ItemViewCount中的windowEnd进行keyBy()操作。然后使用ProcessFunction实现一个自定义的TopN函数TopNHotItems来计算点击量排名前3名的商品，并将排名结果格式化成字符串，便于后续输出。
```scala
.keyBy("windowEnd")
.process(new TopNHotItems(3));  // 求点击量前3名的商品
```
ProcessFunction提供了定时器timer，我们将利用timer来判断何时收齐了某个window下所有商品的点击量数据。由于Watermark的进度是全局的，在processElement方法中，每当收到一条数据ItemViewCount，我们就注册一个windowEnd+1的定时器(Flink框架会自动忽略同一时间的重复注册)。windowEnd+1的定时器被触发时，意味着收到了windowEnd+1的Watermark，即收齐了该windowEnd下的所有商品窗口统计值。我们在onTimer()中处理将收集的所有商品及点击量进行排序，选出TopN，并将排名信息格式化成字符串后进行输出。
使用`ListState<ItemViewCount>`来存储收到的每条ItemViewCount消息，保证在发生故障时，状态数据的不丢失和一致性。ListState是Flink提供的类似Java List接口的State API，它集成了框架的checkpoint机制，自动做到了exactly-once的语义保证。
### 6.更换Kafka作为数据源
```scala
val properties = new Properties()
properties.setProperty("bootstrap.servers", "localhost:9092")
properties.setProperty("group.id", "consumer-group")
properties.setProperty("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer")
properties.setProperty("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer")
properties.setProperty("auto.offset.reset", "latest")
val stream = env
  .addSource(new FlinkKafkaConsumer[String]("hotitems", new SimpleStringSchema(), properties))
```
